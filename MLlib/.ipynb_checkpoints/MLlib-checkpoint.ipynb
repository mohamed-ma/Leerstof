{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a82620d",
   "metadata": {},
   "source": [
    "# MLlib\n",
    "\n",
    "Spark biedt ook een framework aan voor MachineLearning modellen te trainen op gedistribueerde datasets.\n",
    "Dit framework is MLlib of ook wel sparkML genoemd.\n",
    "De code om te werken met deze package is sterk gelijkaardig aan sklearn.\n",
    "De API en een uitgebreide documentatie met voorbeeldcode kan je [hier](https://spark.apache.org/docs/latest/ml-guide.html) vinden.\n",
    "\n",
    "Deze package bied de volgende tools aan\n",
    "* ML-technieken: classificatie, regressie, clustering, ...\n",
    "* Features: Extracting en transforming van features, PCA, ...\n",
    "* Pipelines: Maak, train, optimaliseer en evalueer pipelines\n",
    "* Persistentie: Bewaar en laden van algoritmes/modellen\n",
    "* Databeheer: Algebra tools, statistieken, null-waarden, ...\n",
    "\n",
    "Let op dat er twee API's aangeboden worden, 1 gebaseerd op RDD's en 1 op DataFrames.\n",
    "De API gebaseerd op RDD's is ouder en minder flexibel dan de API gebruik makend van DataFrames.\n",
    "Momenteel werken ze allebei maar in de toekomst zou de RDD gebaseerde kunnen verdwijnen.\n",
    "\n",
    "## Utilities\n",
    "\n",
    "### Varianten voor numpy-arrays\n",
    "\n",
    "Voor feature sets en volledige matrices van datasets aan te maken kan je gebruik maken van de Vector en Matrix klassen.\n",
    "Deze beschikken over een Dense variant waar je elk element moet ingeven of een Sparse Variant waar cellen, elementen leeg kan laten.\n",
    "Dit ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef759da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[1,3],[20.0,40.0])\n",
      "[10.0,20.0,30.0,40.0]\n",
      "DenseMatrix([[ 0.,  4.,  8., 12., 16.],\n",
      "             [ 1.,  5.,  9., 13., 17.],\n",
      "             [ 2.,  6., 10., 14., 18.],\n",
      "             [ 3.,  7., 11., 15., 19.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, Matrices\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MLlib les\").getOrCreate()\n",
    "\n",
    "# 1ste index heeft de waarde 20 en 3de index heeft de waarde 40\n",
    "data  = Vectors.sparse(4, [(1, 20.0), (3, 40)])\n",
    "print(data)\n",
    "data = Vectors.dense([10.0, 20.0, 30.0, 40.0])\n",
    "print(data)\n",
    "\n",
    "# 4 rijen, 5 kolommen\n",
    "data = Matrices.dense(4, 5, range(20))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0143ac7",
   "metadata": {},
   "source": [
    "Het is belangrijk om te weten dat dit locale datastructuren (wrapper rond numpy array) zijn en geen gedistribueerde objecten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5676b",
   "metadata": {},
   "source": [
    "### Statistieken\n",
    "\n",
    "Voor er kan gewerkt worden met statistieken moeten we (net zoals bij pandas) eerst een dataset hebben.\n",
    "Hieronder maken we een random dataframe aan van 50 rijen en 4 kolommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91673fc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+--------------------+-------------------+\n",
      "|summary|                 _1|                  _2|                  _3|                 _4|\n",
      "+-------+-------------------+--------------------+--------------------+-------------------+\n",
      "|  count|                 50|                  50|                  50|                 50|\n",
      "|   mean|0.45562355825191936|  0.4645917480810971|  0.5332524347932288|0.42927580322421505|\n",
      "| stddev| 0.2850113494113795|  0.2945111017051597| 0.29985438726453856|0.28703798803216446|\n",
      "|    min|0.01248083893147145|0.026382482425437503|0.022861220918282688|0.00383515376189536|\n",
      "|    max| 0.9347140171756639|  0.9997115724800131|  0.9985109965285944| 0.9415570066595731|\n",
      "+-------+-------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+--------------------+-------------------+\n",
      "|summary|                 _1|                  _2|                  _3|                 _4|\n",
      "+-------+-------------------+--------------------+--------------------+-------------------+\n",
      "|  count|                 50|                  50|                  50|                 50|\n",
      "|   mean|0.45562355825191936|  0.4645917480810971|  0.5332524347932288|0.42927580322421505|\n",
      "| stddev| 0.2850113494113795|  0.2945111017051597| 0.29985438726453856|0.28703798803216446|\n",
      "|    min|0.01248083893147145|0.026382482425437503|0.022861220918282688|0.00383515376189536|\n",
      "|    25%|0.19421395124502927| 0.19483220200892803|  0.2783252273304532|0.19577999724053952|\n",
      "|    50%| 0.4195192842572516|  0.4122525307526367|  0.5625772445702768| 0.3968339578536181|\n",
      "|    75%| 0.7062213780991908|  0.7137135703132439|  0.7647202734224299| 0.6941042848608009|\n",
      "|    max| 0.9347140171756639|  0.9997115724800131|  0.9985109965285944| 0.9415570066595731|\n",
      "+-------+-------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "# 50 rijen, 4 kolommen. we moeten eerst ook alle waardes naar een list veranderen om die te kunnen omzetten in een DF.\n",
    "data = RandomRDDs.uniformVectorRDD(spark.sparkContext, 50, 4).map(lambda a: a.tolist()).toDF()\n",
    "#data.show()\n",
    "data.describe().show()\n",
    "data.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3fad4",
   "metadata": {},
   "source": [
    "**Correlation matrix**\n",
    "\n",
    "Buiten de statistieken die berekend kunnen worden door de summary() functie kan ook de correlatiematrix belangrijk zijn.\n",
    "Deze matrix maakt het mogelijk om het verband tussen de verscheidene features te bestuderen.\n",
    "Deze matrix kan als volgt berekend worden voor een gedistribueerd dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b6287f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pearson(vector)                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0                   -0.20207761350750789  0.21954287038714848   0.10922158175878342   \\n-0.20207761350750789  1.0                   0.016165619046380714  0.018766547940167386  \\n0.21954287038714848   0.016165619046380714  1.0                   0.15597868063215664   \\n0.10922158175878342   0.018766547940167386  0.15597868063215664   1.0                   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eerst alles omzetten naar vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# een assembler zal alle kolommen merge in een vector\n",
    "assembler = VectorAssembler(inputCols = data.columns, outputCol = \"vector\")\n",
    "data_vector = assembler.transform(data)\n",
    "#data.show(5)\n",
    "#data_vector.show(5)\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "df_corr = Correlation.corr(data_vector, \"vector\")\n",
    "df_corr.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0084e1",
   "metadata": {},
   "source": [
    "**Onafhankelijksheidtest**\n",
    "\n",
    "Naast de correlatiematrix kan het ook belangrijk zijn om de onafhankelijkheid te testen tussen elke feature en een label.\n",
    "Dit kan uitgevoerd worden door een zogenaamde ChiSquareTest.\n",
    "Deze krijgt als input een dataframe, de naam van de kolom met de features (als vectors) en de naam van een kolom met de labels.\n",
    "We kunnen deze test uitvoeren als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8412ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|                  _1|                  _2|                  _3|                  _4|              vector|label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|  0.6786402569272866|  0.1875351488551662|  0.7141094941050921| 0.23480925096652705|[0.67864025692728...|    1|\n",
      "|  0.3286454952102197|  0.5940405697895388|  0.5124647148012205|  0.6076959751870075|[0.32864549521021...|    0|\n",
      "|  0.8037966893694491|  0.3062193135304848|  0.6966550546898116|  0.9415570066595731|[0.80379668936944...|    0|\n",
      "|  0.3650753306057878| 0.19483220200892803| 0.08238258562457268| 0.21781337015488966|[0.36507533060578...|    1|\n",
      "|0.021164107524203057|  0.7498019921828105|  0.6946474816967066| 0.39718601540300325|[0.02116410752420...|    1|\n",
      "|  0.3469408072004708|  0.3060211128026944|  0.2784172925138798| 0.20652801111047625|[0.34694080720047...|    0|\n",
      "|  0.5431118974421432| 0.07398073825058082|  0.5970834133382777|  0.5445744430346603|[0.54311189744214...|    0|\n",
      "|  0.5071696788796051|  0.8358496327782287|  0.6243754321754245| 0.34843245247889343|[0.50716967887960...|    0|\n",
      "|  0.7046310419096917|  0.1523249808410354|  0.1879690047189435| 0.05833787030572146|[0.70463104190969...|    1|\n",
      "| 0.28600498233165217|  0.9754709566862783|  0.4593947881692474|  0.7494496903894177|[0.28600498233165...|    0|\n",
      "|  0.5104068366037252|  0.7137135703132439|  0.8703975650019481| 0.00383515376189536|[0.51040683660372...|    0|\n",
      "|  0.9130553247404731|  0.2774837650224887|0.022861220918282688|  0.2552086038314725|[0.91305532474047...|    1|\n",
      "|  0.1455293207926217|  0.5571352388862215| 0.40614702722210994| 0.30889011846517245|[0.14552932079262...|    1|\n",
      "|  0.2060755901780985|  0.4122525307526367| 0.33841000922068676|  0.7399587575036151|[0.20607559017809...|    1|\n",
      "| 0.45326954672960595|  0.4491622827337439|  0.7517301790948225| 0.46089660362668716|[0.45326954672960...|    0|\n",
      "|  0.7760257728527111|  0.2868552943325108|  0.9042773627894847|   0.080399081963761|[0.77602577285271...|    0|\n",
      "|   0.780163755743126|  0.5312008681665739|  0.5625772445702768|  0.5432338233150485|[0.78016375574312...|    1|\n",
      "| 0.15323208389997123| 0.18517243358839264| 0.14286841161982622| 0.07733595323630338|[0.15323208389997...|    1|\n",
      "| 0.01248083893147145|0.026382482425437503|  0.5261921089427225|0.008085901539404916|[0.01248083893147...|    0|\n",
      "|  0.8370831272416867|  0.4214712716774469|  0.8190491986246826| 0.23952495591131662|[0.83708312724168...|    0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.ml.stat.ChiSquareTest.test.\n: java.lang.NullPointerException\n\tat org.apache.spark.ml.stat.ChiSquareTest$.test(ChiSquareTest.scala:75)\n\tat org.apache.spark.ml.stat.ChiSquareTest.test(ChiSquareTest.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5158/2608272504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChiSquareTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata_chi2test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSquareTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vector\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdata_chi2test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/ml/stat.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dataset, featuresCol, labelCol, flatten)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mjavaTestObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChiSquareTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjavaTestObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.ml.stat.ChiSquareTest.test.\n: java.lang.NullPointerException\n\tat org.apache.spark.ml.stat.ChiSquareTest$.test(ChiSquareTest.scala:75)\n\tat org.apache.spark.ml.stat.ChiSquareTest.test(ChiSquareTest.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, when\n",
    "\n",
    "# voeg een label toe op basis van een nieuw willekeurig getal\n",
    "# we gaan kijken of het label afhankelijk is van de features in de vector kolom\n",
    "data_label = data_vector.withColumn(\"label\", when(rand() > .5, 1).otherwise(0)).show()\n",
    "\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "data_chi2test = ChiSquareTest.test(data_label, \"vector\", \"label\")\n",
    "data_chi2test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3640d89",
   "metadata": {},
   "source": [
    "**Summarizer**\n",
    "\n",
    "Andere statistieken per kolom kunnen berekend worden door gebruik te maken van de Summarizer klasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c81a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|aggregate_metrics(vector, 1.0)                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{[0.45562355825191936,0.46459174808109704,0.5332524347932288,0.42927580322421505], 50, [0.9347140171756639,0.9997115724800131,0.9985109965285944,0.9415570066595731]}|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\", \"max\")\n",
    "\n",
    "df_convert = data_vector.select(summarizer.summary(data_vector.vector)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac099f10",
   "metadata": {},
   "source": [
    "Het gebruik maken van de Summarizer maakt het dus mogelijk om rechtstreeks op de feature vectors te werken zonder ze eerst terug te moeten splitsen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e0343",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Pipelines binnen Spark zijn een groep van high-level API's steunend op Dataframes om ML-pipelines aan te maken, optimaliseren en trainen.\n",
    "De belangrijkste concepten binnen de Pipelines van Spark zijn:\n",
    "* Dataframe: concept van de dataset\n",
    "* Transformer: Zet een dataframe om in een ander dataframe\n",
    "* Estimator: Zet een dataframe om in een model/transformer\n",
    "* Pipeline: een ketting van transformers en estimators om een flow vast te leggen\n",
    "* Parameter: API voor parameters van transformers en estimators aan te passen\n",
    "\n",
    "Gebruik nu onderstaande mini-dataset waar we op basis van een tekstkolom met logistische regressie een bepaald label proberen te voorspellen.\n",
    "Maak hiervoor een Pipeline uit die bestaat uit de volgende stappen:\n",
    "* Tokenizer om de tekstkolom te splitsen in de overeenkomstige woorden\n",
    "* HashingTf om de term frequency van de woorden te bepalen en het om te zetten naar een feature vector\n",
    "* LogisticRegression Estimator om de voorspelling te doen.\n",
    "\n",
    "Train daarna deze pipeline en maak de voorspellingen voor de traningsdata.\n",
    "Hoe accuraat is dit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bf87b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+----------------------+----------------------------------------------------------------------------+----------------------------------------+-----------------------------------------+----------+\n",
      "|id |text            |label|words                 |features                                                                    |rawPrediction                           |probability                              |prediction|\n",
      "+---+----------------+-----+----------------------+----------------------------------------------------------------------------+----------------------------------------+-----------------------------------------+----------+\n",
      "|0  |a b c d e spark |1.0  |[a, b, c, d, e, spark]|(262144,[74920,89530,107107,148981,167694,173558],[1.0,1.0,1.0,1.0,1.0,1.0])|[-19.141960614724145,19.141960614724145]|[4.861296721081615E-9,0.9999999951387033]|1.0       |\n",
      "|1  |b d             |0.0  |[b, d]                |(262144,[89530,148981],[1.0,1.0])                                           |[18.80121023262448,-18.80121023262448]  |[0.9999999931650059,6.834994126236893E-9]|0.0       |\n",
      "|2  |spark f g h     |1.0  |[spark, f, g, h]      |(262144,[36803,173558,209078,228158],[1.0,1.0,1.0,1.0])                     |[-19.28078960540036,19.28078960540036]  |[4.231160169977759E-9,0.9999999957688398]|1.0       |\n",
      "|3  |hadoop mapreduce|0.0  |[hadoop, mapreduce]   |(262144,[132966,198017],[1.0,1.0])                                          |[19.805358799142624,-19.805358799142624]|[0.9999999974959559,2.504044083195822E-9]|0.0       |\n",
      "+---+----------------+-----+----------------------+----------------------------------------------------------------------------+----------------------------------------+-----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "# classificatie: zit \"spark\" in de tekst -> 1 is true, 0 is false\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# stap 1 -> splits tekst apart in lists (standaard door spatie)\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "hasher = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "# resultaat = (262144,[74920,89530,107107,148981,167694,173558],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
    "# dit is een sparse vector\n",
    "# 262144 aantal elementen/mogelijke hashes\n",
    "# woord met hash 7490 staat er 1 keer in, 89530 ook 1 keer in, ...\n",
    "# dit is eigenlijk een bag of words\n",
    "lr = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hasher, lr])\n",
    "#hasher.transform(tokenizer.transform(training)).show(truncate=False)\n",
    "model = pipeline.fit(training)\n",
    "predictions = model.transform(training)\n",
    "predictions.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e7af8",
   "metadata": {},
   "source": [
    "### Evalueren van een model\n",
    "\n",
    "In de pyspark.ml package zitten er ook functionaliteiten voor deze modellen te evalueren.\n",
    "Meer informatie hierover vind je [hier](https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd4bd140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evalueren van het model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)\n",
    "# output: 1.0 -> 100% juist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a305ea2",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "\n",
    "Door gebruik te maken van de sparkContext kunnen een reeks standaard databronnen ingelezen worden om datasets uit op te bouwen (Csv, Json, ...).\n",
    "Daarnaast is het ook mogelijk om een folder met een reeks beelden te gebruiken als dataset om zo een model voor image classification te trainen.\n",
    "Download nu [deze](https://www.kaggle.com/returnofsputnik/chihuahua-or-muffin) dataset en upload ze naar een folder op het hadoop filesysteem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7992c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: segmentation\n",
      "Your Kaggle Key: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 183k/183k [00:00<00:00, 3.35MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading chihuahua-or-muffin.zip to ./chihuahua-or-muffin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "import opendatasets as od\n",
    "od.download(\"https://www.kaggle.com/returnofsputnik/chihuahua-or-muffin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8ab5900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload dataset\n",
    "import pydoop.hdfs as hdfs\n",
    "localFS = hdfs.hdfs(host='')\n",
    "client=hdfs.hdfs(host=\"localhost\", port=9000)\n",
    "\n",
    "if not client.exists(\"/user/bigdata/06_Spark\"):\n",
    "    client.create_directory('/user/bigdata/06_Spark')\n",
    "client.set_working_directory('/user/bigdata/06_Spark')\n",
    "\n",
    "localFS.copy(\"chihuahua-or-muffin\", client, \"chihuahua-or-muffin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70542e14",
   "metadata": {},
   "source": [
    "De geuploade images kunnen nu ingelezen worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad93c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               image|\n",
      "+--------------------+\n",
      "|{hdfs://localhost...|\n",
      "|{hdfs://localhost...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n",
      "+-------------------------------------------------------------------------------+------+-----+\n",
      "|origin                                                                         |height|width|\n",
      "+-------------------------------------------------------------------------------+------+-----+\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-4.jpeg  |170   |172  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-7.jpeg  |172   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-1.jpeg  |171   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-8.jpeg  |172   |172  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-6.jpg|169   |172  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-8.jpg|172   |168  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-6.jpeg  |169   |168  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-5.jpg|169   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-2.jpeg  |171   |168  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-5.jpeg  |169   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/muffin-3.jpeg  |170   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-3.jpg|170   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-4.jpg|170   |168  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-7.jpg|172   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-1.jpg|171   |171  |\n",
      "|hdfs://localhost:9000/user/bigdata/06_Spark/chihuahua-or-muffin/chihuahua-2.jpg|171   |172  |\n",
      "+-------------------------------------------------------------------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read images\n",
    "df = spark.read.format(\"image\").option(\"dropInvalid\", True).load(\"06_Spark/chihuahua-or-muffin\")\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df.select(\"image.origin\", \"image.height\", \"image.width\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a67bf4",
   "metadata": {},
   "source": [
    "Merk op dat het werken met images niet zo eenvoudig is.\n",
    "Hiervoor wordt binnen pyspark typisch gebruik gemaakt van de [sparkdl](https://smurching.github.io/spark-deep-learning/site/api/python/sparkdl.html) package.\n",
    "Hierbij staat de dl voor deep learning.\n",
    "Aangezien dit ons momenteel te ver leidt ga ik dit niet verder toelichten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4b653",
   "metadata": {},
   "source": [
    "Een andere aparte databron die eenvoudig ingelezen kan worden is het formaat \"libsvm\".\n",
    "Een bestand van dit formaat wordt ingelezen als een dataframe met twee kolommen: een label en een kolom met de feature-vectors.\n",
    "De code om dergelijk bestand in te laden is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"libsvm\").load(\"{path to file here}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
